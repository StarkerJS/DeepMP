{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperopt - hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from keras import layers, models\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import random\n",
    "from keras import optimizers\n",
    "from keras.layers import SimpleRNN, Dense, Dropout, Activation\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import Callback, ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    framesize  = 19320\n",
    "    inputshape = 230\n",
    "    dirname='../../DATASET/Train/Zero/0.6/AbsoluteTRAIN/'\n",
    "    if dirname[-1]!='/':\n",
    "        dirname=dirname+'/'\n",
    "    listfile=os.listdir(dirname)\n",
    "    X = []\n",
    "    Y = []\n",
    "    for file in listfile:\n",
    "        if \"_\" in file:\n",
    "            continue\n",
    "        wordname=file \n",
    "        textlist=os.listdir(dirname+wordname)\n",
    "        k=0\n",
    "        for text in textlist:\n",
    "            if \"DS_\" in text:\n",
    "                continue\n",
    "            textname=dirname+wordname+\"/\"+text\n",
    "            numbers=[]\n",
    "            with open(textname, mode = 'r') as t:\n",
    "                numbers = [float(num) for num in t.read().split()]\n",
    "                while numbers[0] == 0:\n",
    "                    numbers = numbers[1:]\n",
    "                for i in range(len(numbers),framesize):\n",
    "                    numbers.extend([0.000]) \n",
    "            row=0\n",
    "            landmark_frame=[]\n",
    "            for i in range(0,inputshape):\n",
    "                landmark_frame.extend(numbers[row:row+84])\n",
    "                row += 84\n",
    "            landmark_frame=np.array(landmark_frame)\n",
    "            landmark_frame=landmark_frame.reshape(-1,84)\n",
    "            X.append(np.array(landmark_frame))         \n",
    "            Y.append(wordname)\n",
    "    X=np.array(X)\n",
    "    Y=np.array(Y)\n",
    "    tmp = [[x,y] for x, y in zip(X, Y)]\n",
    "    random.shuffle(tmp)\n",
    "    X = [n[0] for n in tmp]\n",
    "    Y = [n[1] for n in tmp]\n",
    "    k=set(Y)\n",
    "    ks=sorted(k)\n",
    "    text=\"\"\n",
    "    for i in ks:\n",
    "        text=text+i+\" \"\n",
    "    s = Tokenizer()\n",
    "    s.fit_on_texts([text])\n",
    "    encoded=s.texts_to_sequences([Y])[0]\n",
    "    one_hot = to_categorical(encoded)\n",
    "    (x_train, y_train) = X, one_hot\n",
    "    x_train=np.array(x_train)\n",
    "    y_train=np.array(y_train)\n",
    "    \n",
    "    dirnameval='../../DATASET/Train/Zero/0.6/AbsoluteVAL/'\n",
    "    listfile=os.listdir(dirnameval)\n",
    "    XT = []\n",
    "    YT = []\n",
    "    for file in listfile:\n",
    "        if \"_\" in file:\n",
    "            continue\n",
    "        wordname=file\n",
    "        textlist=os.listdir(dirnameval+wordname)\n",
    "        for text in textlist:\n",
    "            if \"DS_\" in text:\n",
    "                continue\n",
    "            textname=dirnameval+wordname+\"/\"+text\n",
    "            numbers=[]\n",
    "            with open(textname, mode = 'r') as t:\n",
    "                numbers = [float(num) for num in t.read().split()]\n",
    "                while numbers[0] == 0:\n",
    "                    numbers = numbers[1:]\n",
    "                for i in range(len(numbers),framesize):\n",
    "                    numbers.extend([0.000]) \n",
    "            landmark_frame=[]\n",
    "            row=0\n",
    "            for i in range(0,inputshape):\n",
    "                landmark_frame.extend(numbers[row:row+84])\n",
    "                row += 84\n",
    "            landmark_frame=np.array(landmark_frame)\n",
    "            landmark_frame=landmark_frame.reshape(-1,84)\n",
    "            XT.append(np.array(landmark_frame))\n",
    "            YT.append(wordname)\n",
    "    XT=np.array(XT)\n",
    "    YT=np.array(YT)\n",
    "\n",
    "    tmp1 = [[xt,yt] for xt, yt in zip(XT, YT)]\n",
    "    random.shuffle(tmp1)\n",
    "\n",
    "    XT = [n[0] for n in tmp1]\n",
    "    YT = [n[1] for n in tmp1]\n",
    "    \n",
    "    k=set(YT)\n",
    "    ks=sorted(k)\n",
    "    text=\"\"\n",
    "    for i in ks:\n",
    "        text=text+i+\" \"\n",
    "    \n",
    "    s = Tokenizer()\n",
    "    s.fit_on_texts([text])\n",
    "    encoded1=s.texts_to_sequences([YT])[0]\n",
    "    one_hot2=to_categorical(encoded1)\n",
    "    \n",
    "    (x_test,y_test)=XT,one_hot2\n",
    "    x_test=np.array(x_test)\n",
    "    y_test=np.array(y_test)\n",
    "\n",
    "    return x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    epoch      = 300\n",
    "    batchsize  = 40\n",
    "    model = Sequential()\n",
    "    model.add(layers.LSTM({{choice([32, 64, 128, 256, 512])}}, return_sequences=True, input_shape=(inputshape, 84)))\n",
    "    if {{choice(['two', 'three', 'four', 'five'])}} == 'two':\n",
    "        pass\n",
    "    elif {{choice(['two', 'three', 'four', 'five'])}} == 'three':\n",
    "        model.add(layers.LSTM({{choice([32, 64, 128, 256, 512])}}, return_sequences=True))\n",
    "    elif {{choice(['two', 'three', 'four', 'five'])}} == 'four':\n",
    "        model.add(layers.LSTM({{choice([32, 64, 128, 256, 512])}}, return_sequences=True))\n",
    "        model.add(layers.LSTM({{choice([32, 64, 128, 256, 512])}}, return_sequences=True))\n",
    "    elif {{choice(['two', 'three', 'four', 'five'])}} == 'five':\n",
    "        model.add(layers.LSTM({{choice([32, 64, 128, 256, 512])}}, return_sequences=True))\n",
    "        model.add(layers.LSTM({{choice([32, 64, 128, 256, 512])}}, return_sequences=True))\n",
    "        model.add(layers.LSTM({{choice([32, 64, 128, 256, 512])}}, return_sequences=True))\n",
    "    \n",
    "    model.add(layers.LSTM({{choice([32, 64, 128, 256, 512])}}))\n",
    "    model.add(layers.Dense(14,activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batchsize,\n",
    "              epochs=epoch,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    val_loss, val_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return {'loss': -val_loss, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print('Training stage')\n",
    "    best_run, best_model = optim.minimize(model=create_model,\n",
    "                                          data=prepare_data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=100,\n",
    "                                          notebook_name='Hyperopt',\n",
    "                                          trials=Trials())\n",
    "    print(best_model.summary())\n",
    "    print(best_run)\n",
    "    best_model.save('best_model.h5')\n",
    "    _, _, x_test, y_test = prepare_data()\n",
    "    val_loss, val_acc = best_model.evaluate(x_test,y_test,batch_size=40,verbose=0)\n",
    "    print(\"val_loss: \", val_loss)\n",
    "    print(\"val_acc: \", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
