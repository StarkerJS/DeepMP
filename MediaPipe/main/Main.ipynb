{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout-method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from keras import layers, models\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import random\n",
    "from keras import optimizers\n",
    "from keras.layers import SimpleRNN, Dense, Dropout\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import Callback, ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pydot\n",
    "import graphviz\n",
    "plt.rcParams[\"font.family\"] = \"TakaoGothic\"\n",
    "plt.rc('figure',figsize=[8.0, 4.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label(text):\n",
    "    with open(\"label.txt\", \"w\") as f:\n",
    "         f.write(text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_zero_sequences_endlist(txtlist):\n",
    "    numbers = txtlist\n",
    "    if (numbers[-1] ==0 and numbers[-2] ==0 and numbers[-3] ==0 and numbers[-4] ==0 and numbers[-5] ==0 and numbers[-6] ==0\n",
    "    and numbers[-7] ==0 and numbers[-8] ==0 and numbers[-9] ==0 and numbers[-10] ==0 and numbers[-11] ==0 and numbers[-12] ==0\n",
    "    and numbers[-13] ==0 and numbers[-14] ==0 and numbers[-15] ==0 and numbers[-16] ==0 and numbers[-17] ==0 and numbers[-18] ==0\n",
    "    and numbers[-19] ==0 and numbers[-20] ==0 and numbers[-21] ==0 and numbers[-22] ==0 and numbers[-23] ==0 and numbers[-24] ==0\n",
    "    and numbers[-25] ==0 and numbers[-26] ==0 and numbers[-27] ==0 and numbers[-28] ==0 and numbers[-29] ==0 and numbers[-30] ==0\n",
    "    and numbers[-31] ==0 and numbers[-32] ==0 and numbers[-33] ==0 and numbers[-34] ==0 and numbers[-35] ==0 and numbers[-36] ==0\n",
    "    and numbers[-37] ==0 and numbers[-38] ==0 and numbers[-39] ==0 and numbers[-40] ==0 and numbers[-41] ==0 and numbers[-42] ==0):\n",
    "        numbers = numbers[:-42]\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN OPTION\n",
    "framesize  = 19320  #23-*84 18900\n",
    "inputshape = 230   #126(180) #70(100) 35(50)\n",
    "epoch      = 300\n",
    "batchsize  = 40\n",
    "model_note = 'LSTM'\n",
    "tenb_path  = './SaveData/logs/' + model_note\n",
    "save_path  = 'SaveData/'+ model_note +'/'\n",
    "os.makedirs(save_path,exist_ok=True)\n",
    "\n",
    "traindata  = '../../DATASET/Train/Zero/0.6/AbsoluteTRAIN/'\n",
    "valdata    = '../../DATASET/Train/Zero/0.6/AbsoluteVAL/'\n",
    "#TEST OPTION\n",
    "testdatadir= '../../DATASET/Train/Zero/0.6/AbsoluteTEST/'\n",
    "classname  = ['おはよう','どういたしまして','好き','嫌い','おめでとう','新しい','こんばんは','久しぶり','ありがとう','元気','自由','ウサギ','負け']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(label):\n",
    "    model = Sequential()\n",
    "    model.add(layers.LSTM(256, return_sequences=True,input_shape=(inputshape, 84))) \n",
    "    #model.add(layers.LSTM(512, return_sequences=True))\n",
    "    #model.add(layers.LSTM(256, return_sequences=True))\n",
    "    model.add(layers.LSTM(128))\n",
    "    model.add(layers.Dense(label, activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 検証アリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dirname):\n",
    "    if dirname[-1]!='/':\n",
    "        dirname=dirname+'/'\n",
    "    listfile=os.listdir(dirname) #入出力のファイルリスト\n",
    "    X = []\n",
    "    Y = []\n",
    "    for file in listfile: #単語名を検出\n",
    "        if \"_\" in file:   #ビデオファイルを無視\n",
    "            continue\n",
    "        wordname=file \n",
    "        textlist=os.listdir(dirname+wordname)  # テキストファイルリスト\n",
    "        k=0\n",
    "        for text in textlist: #\n",
    "            if \"DS_\" in text:\n",
    "                continue\n",
    "            textname=dirname+wordname+\"/\"+text\n",
    "            numbers=[]\n",
    "            #print(textname)\n",
    "            with open(textname, mode = 'r') as t:\n",
    "                numbers = [float(num) for num in t.read().split()] #テキストファイル内のポイントを切り取り\n",
    "                #print(len(numbers))\n",
    "                while numbers[0] == 0:\n",
    "                    numbers = numbers[1:]\n",
    "                for i in range(len(numbers),framesize): #50frame * 84 = 4200     60*3*84=15120  250*84=21000\n",
    "                    numbers.extend([0.000]) \n",
    "            row=0\n",
    "            landmark_frame=[]\n",
    "            for i in range(0,inputshape): #100frame = 70, 50frame = 35     126\n",
    "                landmark_frame.extend(numbers[row:row+84])\n",
    "                row += 84\n",
    "            #print(len(landmark_frame))\n",
    "            landmark_frame=np.array(landmark_frame) #35*84*2 (5880,) 1dim\n",
    "            landmark_frame=landmark_frame.reshape(-1,84) #(70,84) 2dim\n",
    "            X.append(np.array(landmark_frame))         \n",
    "            Y.append(wordname)\n",
    "            \n",
    "    X=np.array(X)\n",
    "    Y=np.array(Y)\n",
    "    \n",
    "    tmp = [[x,y] for x, y in zip(X, Y)]\n",
    "    random.shuffle(tmp)\n",
    "    \n",
    "    X = [n[0] for n in tmp]\n",
    "    Y = [n[1] for n in tmp]\n",
    "    \n",
    "    k=set(Y)\n",
    "    ks=sorted(k)\n",
    "    text=\"\"\n",
    "    for i in ks:\n",
    "        text=text+i+\" \"\n",
    "    make_label(text)\n",
    "    \n",
    "    s = Tokenizer()\n",
    "    s.fit_on_texts([text])\n",
    "    encoded=s.texts_to_sequences([Y])[0]\n",
    "    one_hot = to_categorical(encoded)\n",
    "    \n",
    "\n",
    "    (x_train, y_train) = X, one_hot\n",
    "    x_train=np.array(x_train)\n",
    "    y_train=np.array(y_train)\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_valdata(dirname):\n",
    "    listfile=os.listdir(dirname)\n",
    "    XT = []\n",
    "    YT = []\n",
    "    for file in listfile:\n",
    "        if \"_\" in file:\n",
    "            continue\n",
    "        wordname=file\n",
    "        textlist=os.listdir(dirname+wordname)\n",
    "        for text in textlist:\n",
    "            if \"DS_\" in text:\n",
    "                continue\n",
    "            textname=dirname+wordname+\"/\"+text\n",
    "            numbers=[]\n",
    "            with open(textname, mode = 'r') as t:\n",
    "                numbers = [float(num) for num in t.read().split()]\n",
    "                while numbers[0] == 0:\n",
    "                    numbers = numbers[1:]\n",
    "                for i in range(len(numbers),framesize):\n",
    "                    numbers.extend([0.000]) \n",
    "            landmark_frame=[]\n",
    "            row=0\n",
    "            for i in range(0,inputshape):\n",
    "                landmark_frame.extend(numbers[row:row+84])\n",
    "                row += 84\n",
    "            landmark_frame=np.array(landmark_frame)\n",
    "            landmark_frame=landmark_frame.reshape(-1,84)\n",
    "            XT.append(np.array(landmark_frame))\n",
    "            YT.append(wordname)\n",
    "    XT=np.array(XT)\n",
    "    YT=np.array(YT)\n",
    "\n",
    "    tmp1 = [[xt,yt] for xt, yt in zip(XT, YT)]\n",
    "    random.shuffle(tmp1)\n",
    "\n",
    "    XT = [n[0] for n in tmp1]\n",
    "    YT = [n[1] for n in tmp1]\n",
    "    \n",
    "    k=set(YT)\n",
    "    ks=sorted(k)\n",
    "    text=\"\"\n",
    "    for i in ks:\n",
    "        text=text+i+\" \"\n",
    "    #make_label(text)\n",
    "    \n",
    "    s = Tokenizer()\n",
    "    s.fit_on_texts([text])\n",
    "    encoded1=s.texts_to_sequences([YT])[0]\n",
    "    one_hot2=to_categorical(encoded1)\n",
    "    \n",
    "    (x_test,y_test)=XT,one_hot2\n",
    "    x_test=np.array(x_test)\n",
    "    y_test=np.array(y_test)\n",
    "    return x_test,y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデル構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### メインプログラムトレーニングモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(traindirname,valdirname):  \n",
    "    x_train,y_train=load_data(traindirname)\n",
    "    x_test,y_test=load_valdata(valdirname)\n",
    "    #num_val_samples=(x_train.shape[0])//5\n",
    "    model=build_model(y_train.shape[1])\n",
    "    print('Training stage')\n",
    "\n",
    "    #Callbacks\n",
    "    tb = TensorBoard(log_dir = tenb_path, histogram_freq=1, write_graph=True)\n",
    "    cp = ModelCheckpoint(save_path+'model.h5', monitor='val_loss', verbose=0, mode = 'auto', save_best_only=True, save_weights_only=False)\n",
    "    es = EarlyStopping(monitor='val_loss', patience=35, verbose=0, mode='auto', restore_best_weights=True)\n",
    "    cbks = [tb,cp]   #tb,cp,es\n",
    "    \n",
    "    \n",
    "    #history = model.fit(x_train, y_train, batch_size=batch_size, epochs = epochs, verbose=1, callbacks=cbks, validation_data=(x_test,y_test))\n",
    "    history = model.fit(x_train, y_train, epochs=epoch, batch_size=batchsize, verbose=0, validation_data=(x_test,y_test), callbacks = cbks)\n",
    "    print('_________________________________________________________________')\n",
    "    print('Shape & history')\n",
    "    print(x_train.shape, x_test.shape)\n",
    "    print(y_train.shape, y_test.shape)\n",
    "    \n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    print('_________________________________________________________________')\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylim(0,1.1)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.savefig(save_path+'accuracy.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylim(0,3.0)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.savefig(save_path+'loss.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    plot_model(model,show_shapes=True,to_file=save_path+'model.png',expand_nested=True,dpi=300)\n",
    "    model.summary()\n",
    "    print('_________________________________________________________________')\n",
    "    print('Test stage')\n",
    "     \n",
    "    score = model.evaluate(x_test,y_test,batch_size=batchsize,verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    yhat = model.predict(x_test)\n",
    "    clf_report = classification_report(np.argmax(y_test,axis=1), np.argmax(yhat, axis=1),target_names=classname,output_dict=True,digits=5)\n",
    "    sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T,  cmap=\"PuBu\", annot=True, vmax=1, vmin=0, linecolor=\"white\", linewidths=.5,xticklabels=True, yticklabels=True,fmt=\".5f\")\n",
    "    plt.savefig(save_path+'train_classification.png',dpi=300,bbox_inches=\"tight\")\n",
    "    \n",
    "    print('_________________________________________________________________')\n",
    "    print('Shape & history')\n",
    "    print(x_train.shape, x_test.shape)\n",
    "    print(y_train.shape, y_test.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(traindata,valdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix on test set (テストセット混同行列)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_testdata(dirname):\n",
    "    listfile=os.listdir(dirname)\n",
    "    XT = []\n",
    "    YT = []\n",
    "    for file in listfile:\n",
    "        if \"_\" in file:\n",
    "            continue\n",
    "        wordname=file\n",
    "        textlist=os.listdir(dirname+wordname)\n",
    "        for text in textlist:\n",
    "            if \"DS_\" in text:\n",
    "                continue\n",
    "            textname=dirname+wordname+\"/\"+text\n",
    "            numbers=[]\n",
    "            with open(textname, mode = 'r') as t:\n",
    "                numbers = [float(num) for num in t.read().split()]\n",
    "                while numbers[0] == 0:\n",
    "                    numbers = numbers[1:]\n",
    "                for i in range(len(numbers),framesize):\n",
    "                    numbers.extend([0.000]) \n",
    "            landmark_frame=[]\n",
    "            row=0\n",
    "            for i in range(0,inputshape):\n",
    "                landmark_frame.extend(numbers[row:row+84])\n",
    "                row += 84\n",
    "            landmark_frame=np.array(landmark_frame)\n",
    "            landmark_frame=landmark_frame.reshape(-1,84)\n",
    "            XT.append(np.array(landmark_frame))\n",
    "            YT.append(wordname)\n",
    "    XT=np.array(XT)\n",
    "    YT=np.array(YT)\n",
    "\n",
    "    tmp1 = [[xt,yt] for xt, yt in zip(XT, YT)]\n",
    "    random.shuffle(tmp1)\n",
    "\n",
    "    XT = [n[0] for n in tmp1]\n",
    "    YT = [n[1] for n in tmp1]\n",
    "    \n",
    "    k=set(YT)\n",
    "    ks=sorted(k)\n",
    "    text=\"\"\n",
    "    for i in ks:\n",
    "        text=text+i+\" \"\n",
    "    #make_label(text)\n",
    "    \n",
    "    s = Tokenizer()\n",
    "    s.fit_on_texts([text])\n",
    "    encoded1=s.texts_to_sequences([YT])[0]\n",
    "    one_hot2=to_categorical(encoded1)\n",
    "    \n",
    "    (x_test,y_test)=XT,one_hot2\n",
    "    x_test=np.array(x_test)\n",
    "    y_test=np.array(y_test)\n",
    "    return x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = testdatadir\n",
    "x_test,y_test=load_testdata(dirname)\n",
    "new_model = tf.keras.models.load_model(save_path+'model.h5')\n",
    "new_model.summary()\n",
    "\n",
    "print('Test stage')\n",
    "score = new_model.evaluate(x_test,y_test,batch_size=batchsize,verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('_________________________________________________________________')\n",
    "print('Shape & history')\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "xhat = x_test\n",
    "yhat = new_model.predict(xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yhat)\n",
    "print(yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.PuBu,linecolor=\"white\", linewidths=10):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size=40)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize = 30)\n",
    "    plt.yticks(tick_marks, classes, fontsize = 30)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 25)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize = 25)\n",
    "    plt.xlabel('Predicted label', fontsize = 25)\n",
    "    plt.savefig(save_path+'matrix.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(np.argmax(y_test,axis=1), np.argmax(yhat, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,20))\n",
    "class_names = classname\n",
    "\n",
    "plot_confusion_matrix(cfm, classes=class_names, title='')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_report = classification_report(np.argmax(y_test,axis=1), np.argmax(yhat, axis=1),target_names=classname,output_dict=True,digits=5)\n",
    "\n",
    "sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T,  cmap=\"PuBu\", annot=True, vmax=1, vmin=0, linecolor=\"white\", linewidths=.5,xticklabels=True, yticklabels=True,fmt=\".5f\")\n",
    "\n",
    "plt.savefig(save_path+'classification.png',dpi=300,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
